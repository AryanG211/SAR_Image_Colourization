{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTING THE LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATASET CLASS INITIALIASATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SatelliteDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.gray_dir = os.path.join(root_dir, 'gray')\n",
    "        self.color_dir = os.path.join(root_dir, 'color')\n",
    "        \n",
    "        self.image_names = list(set(\n",
    "            [f for f in os.listdir(self.gray_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        ) & set(\n",
    "            [f for f in os.listdir(self.color_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        ))\n",
    "        \n",
    "        self.transform = transform if transform else transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            image_name = self.image_names[idx]\n",
    "            gray_path = os.path.join(self.gray_dir, image_name)\n",
    "            color_path = os.path.join(self.color_dir, image_name)\n",
    "            \n",
    "            gray_image = Image.open(gray_path).convert('RGB')\n",
    "            color_image = Image.open(color_path).convert('RGB')\n",
    "            \n",
    "            gray_tensor = self.transform(gray_image)\n",
    "            color_tensor = self.transform(color_image)\n",
    "            \n",
    "            return gray_tensor, color_tensor\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_name}: {e}\")\n",
    "            dummy = torch.zeros(3, 256, 256)\n",
    "            return dummy, dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRINT PROGRESS FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_progress(current, total, prefix='', suffix='', decimals=1, length=50, fill='█', print_end=\"\\r\"):\n",
    "    \n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (current / float(total)))\n",
    "    filled_length = int(length * current // total)\n",
    "    bar = fill * filled_length + '-' * (length - filled_length)\n",
    "    print(f'\\r{prefix} |{bar}| {percent}% {suffix}', end=print_end)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    # Print New Line on Complete\n",
    "    if current == total: \n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DOWNSAMPLE CLASS INITIALISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownSample(nn.Module):\n",
    "    def __init__(self, Input_Channels, Output_Channels):\n",
    "        super(DownSample, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(Input_Channels, Output_Channels, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UPSAMPLE CLASS INITIALISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Upsample(nn.Module):\n",
    "    def __init__(self, Input_Channels, Output_Channels):\n",
    "        super(Upsample, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.ConvTranspose2d(Input_Channels, Output_Channels, 4, 2, 1, bias=False),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip_input):\n",
    "        x = self.model(x)\n",
    "        x = torch.cat((x, skip_input), 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATION OF GENERATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # Downsampling path\n",
    "        self.down1 = DownSample(in_channels, 64)\n",
    "        self.down2 = DownSample(64, 128)\n",
    "        self.down3 = DownSample(128, 256)\n",
    "        self.down4 = DownSample(256, 512)\n",
    "        self.down5 = DownSample(512, 512)\n",
    "        self.down6 = DownSample(512, 512)\n",
    "        self.down7 = DownSample(512, 512)\n",
    "        self.down8 = DownSample(512, 512)\n",
    "        \n",
    "        # Upsampling path\n",
    "        self.up1 = Upsample(512, 512)\n",
    "        self.up2 = Upsample(1024, 512)\n",
    "        self.up3 = Upsample(1024, 512)\n",
    "        self.up4 = Upsample(1024, 512)\n",
    "        self.up5 = Upsample(1024, 256)\n",
    "        self.up6 = Upsample(512, 128)\n",
    "        self.up7 = Upsample(256, 64)\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.ZeroPad2d((1,0,1,0)),\n",
    "            nn.Conv2d(128, 3, 4, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Downsampling\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        d5 = self.down5(d4)\n",
    "        d6 = self.down6(d5)\n",
    "        d7 = self.down7(d6)\n",
    "        d8 = self.down8(d7)\n",
    "\n",
    "        # Upsampling with skip connections\n",
    "        u1 = self.up1(d8, d7)\n",
    "        u2 = self.up2(u1, d6)\n",
    "        u3 = self.up3(u2, d5)\n",
    "        u4 = self.up4(u3, d4)\n",
    "        u5 = self.up5(u4, d3)\n",
    "        u6 = self.up6(u5, d2)\n",
    "        u7 = self.up7(u6, d1)\n",
    "        u8 = self.final(u7)\n",
    "        \n",
    "        return u8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATION OF DISCRIMINATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(6, 64, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.ZeroPad2d((1,0,1,0)),\n",
    "            nn.Conv2d(512, 1, 4, padding=1, bias=False)\n",
    "        )\n",
    "        \n",
    "    def forward(self, img_A, img_B):\n",
    "        img_input = torch.cat((img_A, img_B), 1)\n",
    "        return self.model(img_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRININT PROGRESS FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_progress(current, total, prefix='', suffix='', decimals=1, length=50, fill='█', print_end=\"\\r\"):\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (current / float(total)))\n",
    "    filled_length = int(length * current // total)\n",
    "    bar = fill * filled_length + '-' * (length - filled_length)\n",
    "    print(f'\\r{prefix} |{bar}| {percent}% {suffix}', end=print_end)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    if current == total: \n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DISPLAY OF THE IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_and_display_images(inputs, generated_images, targets, epoch):\n",
    "    # Create output directory\n",
    "    os.makedirs('output', exist_ok=True)\n",
    "    \n",
    "    inputs = inputs.detach().cpu()\n",
    "    generated_images = generated_images.detach().cpu()\n",
    "    targets = targets.detach().cpu()\n",
    "    \n",
    "    # Denormalize images from [-1,1] to [0,1]\n",
    "    inputs = (inputs + 1) / 2.0\n",
    "    generated_images = (generated_images + 1) / 2.0\n",
    "    targets = (targets + 1) / 2.0\n",
    "    \n",
    "    # Select first 4 images from the batch\n",
    "    num_images = min(4, inputs.size(0))\n",
    "    \n",
    "    # Create a figure with 3 columns per row\n",
    "    plt.figure(figsize=(15, 5 * num_images))\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        # Input (Grayscale)\n",
    "        plt.subplot(num_images, 3, 3*i + 1)\n",
    "        plt.title(f'Epoch {epoch}: Input (Grayscale)')\n",
    "        plt.imshow(transforms.ToPILImage()(inputs[i]))\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Generated Image\n",
    "        plt.subplot(num_images, 3, 3*i + 2)\n",
    "        plt.title(f'Epoch {epoch}: Generated')\n",
    "        plt.imshow(transforms.ToPILImage()(generated_images[i]))\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Target Image\n",
    "        plt.subplot(num_images, 3, 3*i + 3)\n",
    "        plt.title(f'Epoch {epoch}: Target')\n",
    "        plt.imshow(transforms.ToPILImage()(targets[i]))\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    plt.savefig(f'output/images_epoch_{epoch}.png')\n",
    "    print(f\"Images for epoch {epoch} saved to output/images_epoch_{epoch}.png\")\n",
    "    \n",
    "    # Show the plot \n",
    "    plt.show(block=False)\n",
    "    plt.pause(2)  \n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1_lambda = 10\n",
    "NUM_EPOCHS = 150  \n",
    "lr = 0.0001\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "batch_size = 4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATASET AND DATALOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = r\"C:\\Users\\hp\\OneDrive\\Desktop\\SAR_Dataset\\Patching\\Patching\"  \n",
    "dataset = SatelliteDataset(dataset_path)\n",
    "dataloader_train = torch.utils.data.DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "total_batches = len(dataloader_train)\n",
    "print(f\"Total batches per epoch: {total_batches}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INITIALISING GENERATOR AND DISCRIMINATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Discriminator().to(device)\n",
    "generator = Generator().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INITIALIZATION OF LOSS CALCULATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversarial_loss = nn.BCEWithLogitsLoss()\n",
    "l1_loss = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INITIALISING THE OPTIMIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_opt = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "generator_opt = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "disc_losses = []\n",
    "gen_losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_losses = []\n",
    "gen_losses = []\n",
    "\n",
    "# Training loop (your existing code remains the same)\n",
    "checkpoint_folder = \"checkpoints_10\"\n",
    "os.makedirs(checkpoint_folder, exist_ok=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(NUM_EPOCHS + 1):\n",
    "    # Reset epoch metrics\n",
    "    total_disc_loss = 0\n",
    "    total_gen_loss = 0\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    \n",
    "    # Iterate through batches\n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloader_train):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Discriminator training\n",
    "        discriminator_opt.zero_grad()\n",
    "        \n",
    "        # Real images\n",
    "        real_output = discriminator(inputs, targets)\n",
    "        real_labels = torch.ones_like(real_output)\n",
    "        real_loss = adversarial_loss(real_output, real_labels)\n",
    "\n",
    "        # Fake images\n",
    "        generated_image = generator(inputs)\n",
    "        fake_output = discriminator(inputs, generated_image.detach())\n",
    "        fake_labels = torch.zeros_like(fake_output)\n",
    "        fake_loss = adversarial_loss(fake_output, fake_labels)\n",
    "\n",
    "        disc_loss = (real_loss + fake_loss) / 2\n",
    "        disc_loss.backward()\n",
    "        discriminator_opt.step()\n",
    "\n",
    "        # Generator training\n",
    "        generator_opt.zero_grad()\n",
    "        fake_output = discriminator(inputs, generated_image)\n",
    "        gen_labels = torch.ones_like(fake_output)\n",
    "        \n",
    "        # Adversarial loss\n",
    "        adv_loss = adversarial_loss(fake_output, gen_labels)\n",
    "        \n",
    "        # L1 loss\n",
    "        l1_reconstruction_loss = l1_loss(generated_image, targets)\n",
    "        \n",
    "        # Total generator loss\n",
    "        gen_loss = adv_loss + L1_lambda * l1_reconstruction_loss\n",
    "        gen_loss.backward()\n",
    "        generator_opt.step()\n",
    "\n",
    "        # Accumulate losses\n",
    "        total_disc_loss += disc_loss.item()\n",
    "        total_gen_loss += gen_loss.item()\n",
    "\n",
    "        # Update progress bar\n",
    "        print_progress(\n",
    "            batch_idx + 1, \n",
    "            total_batches, \n",
    "            prefix='Training:', \n",
    "            suffix=f'Disc Loss: {total_disc_loss/(batch_idx+1):.4f} Gen Loss: {total_gen_loss/(batch_idx+1):.4f}'\n",
    "        )\n",
    "\n",
    "    # Periodic visualization and logging\n",
    "    avg_disc_loss = total_disc_loss / total_batches\n",
    "    avg_gen_loss = total_gen_loss / total_batches\n",
    "    \n",
    "    # Append losses to tracking lists\n",
    "    disc_losses.append(avg_disc_loss)\n",
    "    gen_losses.append(avg_gen_loss)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "    print(f\"Average Discriminator Loss: {avg_disc_loss:.4f}\")\n",
    "    print(f\"Average Generator Loss: {avg_gen_loss:.4f}\")\n",
    "\n",
    "    # Save and display images after each epoch\n",
    "    save_and_display_images(inputs, generated_image, targets, epoch)\n",
    "\n",
    "    # Save model checkpoints every 5 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'generator_state_dict': generator.state_dict(),\n",
    "            'discriminator_state_dict': discriminator.state_dict(),\n",
    "            'generator_optimizer_state_dict': generator_opt.state_dict(),\n",
    "            'discriminator_optimizer_state_dict': discriminator_opt.state_dict(),\n",
    "            'disc_losses': disc_losses,\n",
    "            'gen_losses': gen_losses\n",
    "        }\n",
    "        checkpoint_path = os.path.join(checkpoint_folder, f'checkpoint_epoch_{epoch + 1}.pth')\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"Checkpoint saved at {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GEN AND DISC LOSS VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Discriminator Loss Subplot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(disc_losses) + 1), disc_losses, label='Discriminator Loss', color='blue')\n",
    "plt.title('Discriminator Loss per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Generator Loss Subplot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(gen_losses) + 1), gen_losses, label='Generator Loss', color='red')\n",
    "plt.title('Generator Loss per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Adjust layout and save\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/training_losses.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAVING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(generator.state_dict(), 'Colourized_model.pth')\n",
    "print(\"Model saved successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sih",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
